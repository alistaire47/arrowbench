<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Define a Benchmark — Benchmark • arrowbench</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Define a Benchmark — Benchmark" />
<meta property="og:description" content="Define a Benchmark" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">arrowbench</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Define a Benchmark</h1>
    
    <div class="hidden name"><code>Benchmark.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Define a Benchmark</p>
    </div>

    <pre class="usage"><span class='fu'>Benchmark</span><span class='op'>(</span>
  <span class='va'>name</span>,
  setup <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>...</span><span class='op'>)</span> <span class='fu'><a href='BenchEnvironment.html'>BenchEnvironment</a></span><span class='op'>(</span><span class='va'>...</span><span class='op'>)</span>,
  before_each <span class='op'>=</span> <span class='cn'>TRUE</span>,
  run <span class='op'>=</span> <span class='cn'>TRUE</span>,
  after_each <span class='op'>=</span> <span class='cn'>TRUE</span>,
  teardown <span class='op'>=</span> <span class='cn'>TRUE</span>,
  valid_params <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>params</span><span class='op'>)</span> <span class='va'>params</span>,
  <span class='va'>...</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>name</th>
      <td><p>string identifier for the benchmark, included in results</p></td>
    </tr>
    <tr>
      <th>setup</th>
      <td><p>function having as its arguments the benchmark parameters. See
the <code>Parametrizing benchmarks</code> section. This function is called once
to initialize the benchmark context for a given set of parameters.
It should return <code><a href='BenchEnvironment.html'>BenchEnvironment()</a></code> with any parameter values or resources
that the other expressions will need to run.</p></td>
    </tr>
    <tr>
      <th>before_each</th>
      <td><p>expression that is evaluated before every iteration. You may
not need to define one.</p></td>
    </tr>
    <tr>
      <th>run</th>
      <td><p>expression that executes what we want to measure (and nothing more).
Only code in this function is benchmarked.</p></td>
    </tr>
    <tr>
      <th>after_each</th>
      <td><p>expression evaluated after every iteration. You can put here
assertions about the result of <code>run()</code>--errors in this function will fail
the benchmark (not record results)</p></td>
    </tr>
    <tr>
      <th>teardown</th>
      <td><p>expression evaluated after all iterations are complete. Use this to
clean up any artifacts created, for example. This function may error without
affecting the benchmark results</p></td>
    </tr>
    <tr>
      <th>valid_params</th>
      <td><p>function taking a <code>data.frame</code> of setup parameters and
returning a <code>data.frame</code> of setup parameters. Use this to filter out invalid
combinations of parameters (e.g. writing a Feather file with snappy
compression, which is unsupported)</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>additional attributes or functions, possibly called in <code>setup()</code>.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>A <code>Benchmark</code> object containing these functions</p>
    <h2 class="hasAnchor" id="evaluation"><a class="anchor" href="#evaluation"></a>Evaluation</h2>

    

<p>A <code>Benchmark</code> is evaluated something like:</p><pre><span class='va'>env</span> <span class='op'>&lt;-</span> <span class='va'>bm</span><span class='op'>$</span><span class='fu'>setup</span><span class='op'>(</span>param1 <span class='op'>=</span> <span class='st'>"value"</span>, param2 <span class='op'>=</span> <span class='st'>"value"</span><span class='op'>)</span>
<span class='kw'>for</span> <span class='op'>(</span><span class='va'>i</span> <span class='kw'>in</span> <span class='fu'><a href='https://rdrr.io/r/base/seq.html'>seq_len</a></span><span class='op'>(</span><span class='va'>n_iter</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='fu'><a href='https://rdrr.io/r/base/eval.html'>eval</a></span><span class='op'>(</span><span class='va'>bm</span><span class='op'>$</span><span class='va'>before_each</span>, envir <span class='op'>=</span> <span class='va'>env</span><span class='op'>)</span>
  <span class='fu'><a href='measure.html'>measure</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/eval.html'>eval</a></span><span class='op'>(</span><span class='va'>bm</span><span class='op'>$</span><span class='va'>run</span>, envir <span class='op'>=</span> <span class='va'>env</span><span class='op'>)</span><span class='op'>)</span>
  <span class='fu'><a href='https://rdrr.io/r/base/eval.html'>eval</a></span><span class='op'>(</span><span class='va'>bm</span><span class='op'>$</span><span class='va'>after_each</span>, envir <span class='op'>=</span> <span class='va'>env</span><span class='op'>)</span>
<span class='op'>}</span>
<span class='fu'><a href='https://rdrr.io/r/base/eval.html'>eval</a></span><span class='op'>(</span><span class='va'>bm</span><span class='op'>$</span><span class='va'>teardown</span>, envir <span class='op'>=</span> <span class='va'>env</span><span class='op'>)</span>
</pre>

<p>Benchmarks should run a single combination of parameters. Running across
a range of parameter combinations is handled by the runner, not the functions
in the benchmark object.</p>
    <h2 class="hasAnchor" id="parametrizing-benchmarks"><a class="anchor" href="#parametrizing-benchmarks"></a>Parametrizing benchmarks</h2>

    


<p>When we benchmark, we often want to run our code compared with someone else's
code, or we want to run our code but with different settings. There are a few
types of parameters that get expressed differently.</p>
<p>Function parameters, including both arguments to functions we're benchmarking
and the choice of function themselves, are expressed as arguments to the
<code>setup()</code> function. They should be simple values (strings or numbers) that
can easily be expressed in a configuration object or file. Things like R
functions that are parameters should be mapped to string identifiers and
dereferenced inside the <code>setup()</code> function.</p>
<p>Where appropriate, you should enumerate all possible parameter values and
use <code><a href='https://rdrr.io/r/base/match.arg.html'>match.arg()</a></code> in your <code>setup()</code> function to select and validate. By
listing the possible values in the function signature, <code>run_benchmark</code> can
identify the full parameter space to test by the Cartesian product of the
defaults; this set of default parameters can be filtered down by defining a
<code>valid_params()</code> function for your benchmark. If you do not provide default
parameter values, you will be required to specify them at runtime.</p>
<p>Some global or session parameters are managed outside of the Benchmark object
and do not require handling inside the benchmark's functions.
These are options that would apply to all benchmarks. Currently supported
R session parameters in <code><a href='run_benchmark.html'>run_benchmark()</a></code>:</p><ul>
<li><p><code>lib_path</code>: To test different library versions, install into different
lib directories and provide the directories as the <code>lib_path</code> parameters.
They will be passed to <code><a href='https://rdrr.io/r/base/libPaths.html'>.libPaths()</a></code> at the beginning of each run. The
default <code>lib_path</code> is "latest", which doesn't set a special library path.
That is, by omitting <code>lib_path</code>, you're assuming that packages have been
installed outside of this process.</p></li>
<li><p><code>cpu_count</code>: To restrict the number of threads available for computation,
specify an integer <code>cpu_count</code>. This sets the R <code>Ncpus</code> option, which many
packages follow, and also caps the <code>arrow</code> threadpool size.</p></li>
</ul>

<p>Because these parameters can alter the global session state in unpredictable
ways, when we run benchmarks, we always do so by calling out to a fresh R
subprocess. That way, there is no potential contamination.</p>
<p>Any other R <code><a href='https://rdrr.io/r/base/options.html'>options()</a></code> or environment variables that affect behavior under
test, specific to this Benchmark, can be specified as <code>setup()</code> parameters
and set inside that function. Be sure to restore any previous settings in
the <code>teardown()</code> function.</p>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Neal Richardson, Jonathan Keane.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


